// default config file for ndnrtc headless client

// general parameters
general = {
    log_level = "default";  // all, debug, stat, default, none
    log_file = "ndnrtc-client.log";
    log_path = "/tmp";

    use_fec = true; // [true | false] -- use Forward Error Correction
    use_avsync = true; // [true | false] -- enable synchronization between audio/video streams
    
    ndnnetwork ={
        connect_host = "localhost";
        connect_port = 6363;
    };
};

// configuration for streams that will be published
produce = {
    streams = ({
        type = "video";             // [video | audio] 
        name = "camera";            // video stream name
        segment_size = 1000;        // in bytes
        freshness = {               // freshness (in ms) for various data types
            metadata = 15;          // metadata freshness
            sample = 15;            // sample freshness (audio, video delta)
            sampleKey = 900;        // key sample freshness (video key)
        };
        source = {                  // file from where raw frames will be read
            name = "camera.argb";
            type = "file";          // could be either "file" or "pipe"
        };
        sync = "sound";

        threads = ({    // an array of streams all threads that will be published
            name = "low";                       // thread name
            coder = {                           // encoder parameters
                frame_rate = 30;
                gop = 30;                       //group of picture
                start_bitrate = 1000;
                max_bitrate = 10000;
                encode_height = 405;
                encode_width = 720;
                drop_frames = true;     // whether encoder should drop frames
                                        // to maintain start bitrate
            };
        },
        {
            name = "hi";
            coder = {
                frame_rate = 30;
                gop = 30;
                start_bitrate = 3000;
                max_bitrate = 10000;
                encode_height = 1080;
                encode_width = 1920;
                drop_frames = true;
            };
        });
    },
    {
        type = "video";
        name = "desktop";
        segment_size = 1000;
        source = "desktop.argb";    // this way of defining source defaults to a "file" type
        freshness = 2000;
        sync = "sound";

        threads = ({
            name = "mid";
            coder = {
                frame_rate = 30;
                gop = 30;
                start_bitrate = 1000;
                max_bitrate = 10000;
                encode_height = 720;
                encode_width = 1080;
                drop_frames = true;
            };
        },
        {
            name = "low";
            coder = {
                frame_rate = 30;
                gop = 30;
                start_bitrate = 500;
                max_bitrate = 10000;
                encode_height = 288;
                encode_width = 352;
                drop_frames = true;
            };
        });
    },
    {
        type = "audio";
        name = "sound";
        thread = "pcmu";
        segment_size = 1000;
        freshness = 2000;           // in milliseconds
        codec = "g722";
        capture_device = 0;
    });
};

// configuration for streams that will be fetched
consume = {
    basic = {
        audio = {
            interest_lifetime = 2000; // in millisecond
            jitter_size = 150; // minimal jitter buffer size in milliseconds
        };
        video = {
            interest_lifetime = 2000;
            jitter_size = 150;
        };
        // statistics to gather per stream
        // allowed statistics keywords can be found in statistics.h
        stat_gathering = ({
            name="buffer";  // file name prefix (complete filename will be 
                            // <name>-<producer_name>-<stream_name>.stat)
            statistics= ("jitterPlay", "jitterTar", "dArr"); 
        },
        {
            name="playback";
            statistics= ("framesAcq","lambdaD","drdPrime");
        },
        {
            name="play";
            statistics= ("lambdaD","drdPrime","jitterTar","dArr");
        });
    };

    streams = ({
        type = "video";             // [video | audio] 
        base_prefix = "/ndn/edu/ucla/remap/clientB";
        name = "camera";            // video stream name
        thread_to_fetch = "low";    // exact stream thread to fetch from
                                    // should be the name of one thread in this stream
        sink = "clientB-camera";    // file path where raw decoded frames 
                                    // will be stored (without extension)
                                    // full filename will be:
                                    //      "<sink>-<idx>-<W>x<H>.argb"
                                    // idx is required, as during fetching, 
                                    // consumer may receive different frame 
                                    // resolutions (due to ARC switching between
                                    // differen threads)
    },
    {
        type = "video";
        base_prefix = "/ndn/edu/ucla/remap/clientC";
        name = "camera";
        thread_to_fetch = "mid";
        sink = {
            name = "clientC-camera";    // file name of sink
            type = "file";              // "file", "pipe", "nano". if ommited - "file" by default
            write_frame_info = false;    // writes 512 bytes of frame info (see FrameInfo structure) before each frame
                                         // this works only for sink type "nano" currently
        }
    },
    {
        type = "audio";
        base_prefix = "/ndn/edu/ucla/remap/clientB";
        name = "sound";
        thread_to_fetch = "pcmu";
    },
    {
        type = "audio";
        base_prefix = "/ndn/edu/ucla/remap/clientC";
        name = "sound";
        thread_to_fetch = "pcmu";
    });
};
